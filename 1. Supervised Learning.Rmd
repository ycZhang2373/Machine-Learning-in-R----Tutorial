# Supervised Learning in R

## k-Nearest Neighboors

KNN which stand for K Nearest Neighbor, is a Supervised Machine Learning algorithm that classifies a new data point into the target class, depending on the features of its neighboring data points. 

To use KNN algorithm in R, we must first install the ‘class’ package provided by R. This package has the KNN function in it.

Success and failure modes for KNN:
**KNN generally performs well when:**
- the data are concentrated in the feature space
- the classes appear in separable clusters
**KNN generally performs poorly when:**
- the feature space is high-dimensional (many predictors)
- there are superfluous features (unrelated to calss membreships)

Reference: 
https://www.edureka.co/blog/knn-algorithm-in-r/

```{r}
# install.packages('class')       #Install class package
library(class)                    # Load class package
train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])    # X_train
test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])  # X_test
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))            # y_train
cl_test <- factor(c(rep("s",25), rep("c",25), rep("v",25)))       # y_test
cl_hat = knn(train, test, cl, k = 3, prob=TRUE)                   # k: 'k' in KNN model
str(cl_hat)                                                       # Output
table(cl_test, cl_hat)                                            # cross-tabulate
mean(cl_test != cl_hat)                                           # misclassification error 
```

## Tree-Based Models

Tree-Based models, such as decision tree model can be used to visually represent the “decisions” and are widely used to generate predictions. To use tree-based models algorithm in R, we must first install the 'rpart' package provided by R. This package has the rpart function in it.

Here are some parameters explanations while we are operating 'rpart' to generate tree-based models:

`maxdepth`: The maximum depth level of tree. For single split, you can just set maxdepth argument to 1.
`minsplit`: The minimum number of observations that must exist in a node in order for a split to be attempted.
`minbucket`: The minimum number of observations in any terminal node.
`cp`: Complexity parameter - setting cp to a negative amount ensures that the tree will be fully grown.
`method`:  The method argument defines the algorithms. It can be one of anova, poisson, class or exp. In this case, the target variables is categorical, so you will use the method as class.

Success and failure modes for Tree-Based Models:
**Advanages for Tree-Based Models:**
- Does not require normalization/scaling of data.
- Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent.
- Straightward and easy to explain to company's stakeholders.
**Advanages for Tree-Based Models:**
- Sensitive to small change in the data, which might generate a large change in the structure of the decision tree.
- High time-complexity.
- DInadequate for applying regression and predicting continuous values.

Reference:
https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english
https://www.pluralsight.com/guides/explore-r-libraries:-rpart

```{r}
# install.packages('rpart')       #Install class package
library(rpart)                    # Load class package
train <- data.frame(
  ClaimID = 1:7,
  RearEnd = c(TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE),
  Whiplash = c(TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE),
  Fraud = c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
)
mytree <- rpart(
  Fraud ~ RearEnd + Whiplash,   # y ~ feature1 + feature2 + feature3 + ...
  data = train, 
  method = "class",            
  maxdepth = 1, 
  minsplit = 2, 
  minbucket = 1
)
predict_Fraud = predict(mytree, data=train, type = "class")
table(train$Fraud, predict_Fraud)
```

# Unsupervised Learning

## Principle Component Analysis

The main feature of unsupervised learning algorithms, when compared to classification and regression methods, is that input data are unlabeled (i.e. no labels or classes given) and that the algorithm learns the structure of the data without any assistance. This creates two main differences. First, it allows us to process large amounts of data because the data does not need to be manually labeled. Second, it is difficult to evaluate the quality of an unsupervised algorithm due to the absence of an explicit goodness metric as used in supervised learning.

One of the most common tasks in unsupervised learning is dimensionality reduction. On one hand, dimensionality reduction may help with data visualization. On the other hand, it may help deal with the multicollinearity of our data and prepare the data for a supervised learning method (e.g. decision trees).

To use PCA models algorithm in R, we can use "princomp" provided by R. This package has the "PCA" function in it. 

Success and failure modes for Principle Component Analysis (PCA):
**Advanages for PCA:**
- Removes correlated features
- Improves algorithm performance by reducing overfitting
- Improves visualization by transforming high dimensional data to low dimensional data
**Advanages for PCA:**
- May miss some information as compared to the original list of features.
- Independent principal components are not as readable and interpretable as original features sometimes.

Reference:
https://www.i2tutorials.com/what-are-the-pros-and-cons-of-the-pca/
https://www.datacamp.com/tutorial/pca-analysis-r
https://www.r-bloggers.com/2021/05/principal-component-analysis-pca-in-r/

```{r}
iris1<-iris[,-5]
basePCA<-princomp(iris1)     # Default: center = TRUE, scale. = TRUE
summary(basePCA)
# PCA Visualization
# install.packages('usethis')
library(usethis)
# install.packages('devtools')
library(devtools)
# install_github("vqv/ggbiplot")
library(ggbiplot)
ggbiplot(basePCA)
```

