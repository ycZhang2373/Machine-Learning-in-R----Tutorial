# Supervised Learning in R

## k-Nearest Neighboors

KNN which stand for K Nearest Neighbor, is a Supervised Machine Learning algorithm that classifies a new data point into the target class, depending on the features of its neighboring data points. 

To use KNN algorithm in R, we must first install the ‘class’ package provided by R. This package has the KNN function in it.

Success and failure modes for KNN:
**KNN generally performs well when:**
- the data are concentrated in the feature space
- the classes appear in separable clusters
**KNN generally performs poorly when:**
- the feature space is high-dimensional (many predictors)
- there are superfluous features (unrelated to calss membreships)

Reference: 
https://www.edureka.co/blog/knn-algorithm-in-r/

```{r}
# install.packages('class')       #Install class package
library(class)                    # Load class package
train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])    # X_train
test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])  # X_test
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))            # y_train
cl_test <- factor(c(rep("s",25), rep("c",25), rep("v",25)))       # y_test
cl_hat = knn(train, test, cl, k = 3, prob=TRUE)                   # k: 'k' in KNN model
str(cl_hat)                                                       # Output
table(cl_test, cl_hat)                                            # cross-tabulate
mean(cl_test != cl_hat)                                           # misclassification error 
```

## Tree-Based Models

Tree-Based models, such as decision tree model can be used to visually represent the “decisions” and are widely used to generate predictions. To use tree-based models algorithm in R, we must first install the 'rpart' package provided by R. This package has the rpart function in it.

Here are some parameters explanations while we are operating 'rpart' to generate tree-based models:

`maxdepth`: The maximum depth level of tree. For single split, you can just set maxdepth argument to 1.
`minsplit`: The minimum number of observations that must exist in a node in order for a split to be attempted.
`minbucket`: The minimum number of observations in any terminal node.
`cp`: Complexity parameter - setting cp to a negative amount ensures that the tree will be fully grown.
`method`:  The method argument defines the algorithms. It can be one of anova, poisson, class or exp. In this case, the target variables is categorical, so you will use the method as class.

Success and failure modes for Tree-Based Models:
**Advanages for Tree-Based Models:**
- Does not require normalization/scaling of data.
- Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent.
- Straightward and easy to explain to company's stakeholders.
**Advanages for Tree-Based Models:**
- Sensitive to small change in the data, which might generate a large change in the structure of the decision tree.
- High time-complexity.
- DInadequate for applying regression and predicting continuous values.

Reference:
https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english
https://www.pluralsight.com/guides/explore-r-libraries:-rpart

```{r}
# install.packages('rpart')       #Install class package
library(rpart)                    # Load class package
train <- data.frame(
  ClaimID = 1:7,
  RearEnd = c(TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE),
  Whiplash = c(TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE),
  Fraud = c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
)
mytree <- rpart(
  Fraud ~ RearEnd + Whiplash,   # y ~ feature1 + feature2 + feature3 + ...
  data = train, 
  method = "class",            
  maxdepth = 1, 
  minsplit = 2, 
  minbucket = 1
)
predict_Fraud = predict(mytree, data=train, type = "class")
table(train$Fraud, predict_Fraud)
```
